{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format into set ready for training and eval\n",
    "df = pd.read_csv(\"feat_eng_1800.csv\")\n",
    "\n",
    "# Days in the past\n",
    "days_in_past = 0\n",
    "\n",
    "# [{ID, [entries], label}]\n",
    "data = []\n",
    "X = dict()\n",
    "Y = dict()\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "for id in df[\"ID\"].unique():\n",
    "    X[id] = dict()\n",
    "    Y[id] = dict()\n",
    "    df_id = df[df[\"ID\"] == id]\n",
    "    for day in df_id[\"timestamp\"].dt.floor(\"D\").unique():\n",
    "        \n",
    "        df_id_days = df_id[((df_id[\"timestamp\"].dt.date <= day.date()) & (df_id[\"timestamp\"].dt.date >= (day - timedelta(days=days_in_past)).date()))].sort_values(by=\"timestamp\", ascending=False)\n",
    "        label = df_id_days[\"next_mood\"].iloc[0]\n",
    "        df_id_days = df_id_days.sort_values(by=\"timestamp\", ascending=True).drop(columns=[\"ID\"])\n",
    "\n",
    "        # being an RNN, timestamp shouldn't be needed since the order is the important\n",
    "        df_id_days[\"timestamp\"] = df_id_days[\"timestamp\"].astype(int) // 10**11\n",
    "        df_id_days[\"timestamp\"] = df_id_days[\"timestamp\"] - df_id_days[\"timestamp\"].min()\n",
    "\n",
    "        df_id_days = df_id_days.drop(columns=\"Unnamed: 0\").reset_index(drop=True)\n",
    "\n",
    "        # Using only one value since it will be the next day predicted value\n",
    "        X[id][day] , Y[id][day] = df_id_days.drop(columns=\"next_mood\"), df_id_days[\"next_mood\"][0]\n",
    "\n",
    "        #raw_tuples = list(df_id_days.itertuples(index=False, name=None))\n",
    "        #print(len(raw_tuples[0]))\n",
    "        #data.append({\"ID\": id, \"entries\": raw_tuples, \"label\": label})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1109, 48, 22)\n",
      "(1109,)\n",
      "(159, 48, 22)\n",
      "(159,)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "train_ids = random.sample(list(X.keys()), int(len(X)*0.9))\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for id in X.keys():\n",
    "    is_train = True if id in train_ids else False\n",
    "    for day in X[id].keys():\n",
    "        if is_train:\n",
    "            X_train.append(X[id][day])\n",
    "            Y_train.append(Y[id][day])\n",
    "        else:\n",
    "            X_test.append(X[id][day])\n",
    "            Y_test.append(Y[id][day])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1109, 48, 22])\n",
      "torch.Size([159, 48, 22])\n",
      "torch.Size([1109])\n",
      "torch.Size([159])\n",
      "Training Shape: torch.Size([1109, 48, 22]) torch.Size([1109])\n",
      "Testing Shape: torch.Size([159, 48, 22]) torch.Size([159])\n"
     ]
    }
   ],
   "source": [
    "# Convert to pytorch tensors\n",
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "\n",
    "Y_train_tensors = Variable(torch.Tensor(Y_train))\n",
    "Y_test_tensors = Variable(torch.Tensor(Y_test))\n",
    "\n",
    "print(X_train_tensors.shape)\n",
    "print(X_test_tensors.shape) \n",
    "\n",
    "print(Y_train_tensors.shape)\n",
    "print(Y_test_tensors.shape) \n",
    "\n",
    "# Reshaping to rows, timestamps, features\n",
    "X_train_tensors_final = torch.reshape(X_train_tensors,   \n",
    "                                      (X_train_tensors.shape[0], 48, \n",
    "                                       X_train_tensors.shape[2]))\n",
    "X_test_tensors_final = torch.reshape(X_test_tensors,  \n",
    "                                     (X_test_tensors.shape[0], 48, \n",
    "                                      X_test_tensors.shape[2])) \n",
    "\n",
    "\n",
    "\n",
    "# Apparently there are some nan values in training tensors\n",
    "\n",
    "print(torch.isnan(X_train_tensors_final).any())\n",
    "print(torch.isnan(Y_train_tensors).any())\n",
    "print(torch.isnan(X_test_tensors_final).any())\n",
    "print(torch.isnan(Y_test_tensors).any())\n",
    "\n",
    "\n",
    "\n",
    "# Removing nan\n",
    "X_train_tensors_final = torch.where(torch.isnan(X_train_tensors_final), torch.zeros_like(X_train_tensors_final), X_train_tensors_final)\n",
    "\n",
    "X_test_tensors_final = torch.where(torch.isnan(X_test_tensors_final), torch.zeros_like(X_test_tensors_final), X_test_tensors_final)\n",
    "\n",
    "\n",
    "print(\"Training Shape:\", X_train_tensors_final.shape, Y_train_tensors.shape)\n",
    "print(\"Testing Shape:\", X_test_tensors_final.shape, Y_test_tensors.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes  # output size\n",
    "        self.num_layers = num_layers  # number of recurrent layers in the lstm\n",
    "        self.input_size = input_size  # input size\n",
    "        self.hidden_size = hidden_size  # neurons in each lstm layer\n",
    "        # LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.2 if num_layers > 1 else 0)\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)  # fully connected \n",
    "        self.fc_2 = nn.Linear(128, num_classes)  # fully connected last layer\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # hidden state\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # cell state\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))  # (input, hidden, and cell state)\n",
    "        hn = hn[-1]  # last layer's hidden state\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out)  # first dense\n",
    "        out = self.relu(out)  # relu\n",
    "        out = self.fc_2(out)  # final output\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Training mode\n",
    "        lstm.train() \n",
    "        # Reset gradients\n",
    "        optimiser.zero_grad()\n",
    "        # Forward propagation\n",
    "        outputs = lstm(X_train)\n",
    "        # Training loss\n",
    "        loss = loss_fn(outputs, y_train) \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        torch.nn.utils.clip_grad_norm_(lstm.parameters(), max_norm=1)\n",
    "        optimiser.step() \n",
    "\n",
    "        # Evaluation mode\n",
    "        lstm.eval() \n",
    "        # Disable gradient calc\n",
    "        with torch.no_grad():\n",
    "            # Compute classes and losses\n",
    "            test_preds = lstm(X_test)\n",
    "            test_loss = loss_fn(test_preds, y_test)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, train loss: {loss.item():.5f}, test loss: {test_loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 48.56096, test loss: 44.76292\n",
      "Epoch: 100, train loss: 16.91944, test loss: 14.55593\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()    \u001b[38;5;66;03m# mean-squared error for regression\u001b[39;00m\n\u001b[1;32m     19\u001b[0m optimiser \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(lstm\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m              \u001b[49m\u001b[43mlstm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m              \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m              \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m              \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_tensors_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m              \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m              \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test_tensors_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m              \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_test_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(n_epochs, lstm, optimiser, loss_fn, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, y_train) \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Backpropagate\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     16\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(lstm\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_epochs = 1000 # 1000 epochs\n",
    "learning_rate = 0.001 # 0.001 lr\n",
    "\n",
    "input_size = 22 # number of features\n",
    "hidden_size = 2 # number of features in hidden state\n",
    "num_layers = 1 # number of stacked lstm layers\n",
    "\n",
    "num_classes = 1 # number of output classes \n",
    "\n",
    "lstm = LSTM(num_classes, \n",
    "              input_size, \n",
    "              hidden_size, \n",
    "              num_layers)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimiser = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "training_loop(n_epochs=n_epochs,\n",
    "              lstm=lstm,\n",
    "              optimiser=optimiser,\n",
    "              loss_fn=loss_fn,\n",
    "              X_train=X_train_tensors_final,\n",
    "              y_train=Y_train_tensors,\n",
    "              X_test=X_test_tensors_final,\n",
    "              y_test=Y_test_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
