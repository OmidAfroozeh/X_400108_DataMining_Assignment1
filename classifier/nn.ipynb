{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format into set ready for training and eval\n",
    "df = pd.read_csv(\"feat_eng_1800.csv\")\n",
    "\n",
    "# Days in the past\n",
    "days_in_past = 0\n",
    "\n",
    "# [{ID, [entries], label}]\n",
    "data = []\n",
    "X = dict()\n",
    "Y = dict()\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "for id in df[\"ID\"].unique():\n",
    "    X[id] = dict()\n",
    "    Y[id] = dict()\n",
    "    df_id = df[df[\"ID\"] == id]\n",
    "    for day in df_id[\"timestamp\"].dt.floor(\"D\").unique():\n",
    "        \n",
    "        df_id_days = df_id[((df_id[\"timestamp\"].dt.date <= day.date()) & (df_id[\"timestamp\"].dt.date >= (day - timedelta(days=days_in_past)).date()))].sort_values(by=\"timestamp\", ascending=False)\n",
    "        label = df_id_days[\"next_mood\"].iloc[0]\n",
    "        df_id_days = df_id_days.sort_values(by=\"timestamp\", ascending=True).drop(columns=[\"ID\"])\n",
    "\n",
    "        # being an RNN, timestamp shouldn't be needed since the order is the important\n",
    "        df_id_days[\"timestamp\"] = df_id_days[\"timestamp\"].astype(int) // 10**11\n",
    "        df_id_days[\"timestamp\"] = df_id_days[\"timestamp\"] - df_id_days[\"timestamp\"].min()\n",
    "\n",
    "        df_id_days = df_id_days.drop(columns=\"Unnamed: 0\").reset_index(drop=True)\n",
    "\n",
    "        # Using only one value since it will be the next day predicted value\n",
    "        X[id][day] , Y[id][day] = df_id_days.drop(columns=\"next_mood\"), df_id_days[\"next_mood\"][0]\n",
    "\n",
    "        #raw_tuples = list(df_id_days.itertuples(index=False, name=None))\n",
    "        #print(len(raw_tuples[0]))\n",
    "        #data.append({\"ID\": id, \"entries\": raw_tuples, \"label\": label})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1126, 48, 22)\n",
      "(1126,)\n",
      "(142, 48, 22)\n",
      "(142,)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "train_ids = random.sample(list(X.keys()), int(len(X)*0.9))\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for id in X.keys():\n",
    "    is_train = True if id in train_ids else False\n",
    "    for day in X[id].keys():\n",
    "        if is_train:\n",
    "            X_train.append(X[id][day])\n",
    "            Y_train.append(Y[id][day])\n",
    "        else:\n",
    "            X_test.append(X[id][day])\n",
    "            Y_test.append(Y[id][day])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1126, 48, 22])\n",
      "torch.Size([142, 48, 22])\n",
      "torch.Size([1126])\n",
      "torch.Size([142])\n",
      "Training Shape: torch.Size([1126, 48, 22]) torch.Size([1126])\n",
      "Testing Shape: torch.Size([142, 48, 22]) torch.Size([142])\n"
     ]
    }
   ],
   "source": [
    "# Convert to pytorch tensors\n",
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "\n",
    "Y_train_tensors = Variable(torch.Tensor(Y_train))\n",
    "Y_test_tensors = Variable(torch.Tensor(Y_test))\n",
    "\n",
    "print(X_train_tensors.shape)\n",
    "print(X_test_tensors.shape) \n",
    "\n",
    "print(Y_train_tensors.shape)\n",
    "print(Y_test_tensors.shape) \n",
    "\n",
    "# Reshaping to rows, timestamps, features\n",
    "X_train_tensors_final = torch.reshape(X_train_tensors,   \n",
    "                                      (X_train_tensors.shape[0], 48, \n",
    "                                       X_train_tensors.shape[2]))\n",
    "X_test_tensors_final = torch.reshape(X_test_tensors,  \n",
    "                                     (X_test_tensors.shape[0], 48, \n",
    "                                      X_test_tensors.shape[2])) \n",
    "\n",
    "print(\"Training Shape:\", X_train_tensors_final.shape, Y_train_tensors.shape)\n",
    "print(\"Testing Shape:\", X_test_tensors_final.shape, Y_test_tensors.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes # output size\n",
    "        self.num_layers = num_layers # number of recurrent layers in the lstm\n",
    "        self.input_size = input_size # input size\n",
    "        self.hidden_size = hidden_size # neurons in each lstm layer\n",
    "        # LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.2) # lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) # fully connected \n",
    "        self.fc_2 = nn.Linear(128, num_classes) # fully connected last layer\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # hidden state\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        # cell state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        # propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) # (input, hidden, and internal state)\n",
    "        hn = hn.view(-1, self.hidden_size) # reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) # first dense\n",
    "        out = self.relu(out) # relu\n",
    "        out = self.fc_2(out) # final output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train,\n",
    "                  X_test, y_test):\n",
    "    for epoch in range(n_epochs):\n",
    "        lstm.train()\n",
    "        outputs = lstm.forward(X_train) # forward pass\n",
    "        optimiser.zero_grad() # calculate the gradient, manually setting to 0\n",
    "        # obtain the loss function\n",
    "        loss = loss_fn(outputs, y_train)\n",
    "        loss.backward() # calculates the loss of the loss function\n",
    "        optimiser.step() # improve from loss, i.e backprop\n",
    "        # test loss\n",
    "        lstm.eval()\n",
    "        test_preds = lstm(X_test)\n",
    "        test_loss = loss_fn(test_preds, y_test)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch: %d, train loss: %1.5f, test loss: %1.5f\" % (epoch, \n",
    "                                                                      loss.item(), \n",
    "                                                                      test_loss.item())) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: nan, test loss: nan\n",
      "Epoch: 100, train loss: nan, test loss: nan\n",
      "Epoch: 200, train loss: nan, test loss: nan\n",
      "Epoch: 300, train loss: nan, test loss: nan\n",
      "Epoch: 400, train loss: nan, test loss: nan\n",
      "Epoch: 500, train loss: nan, test loss: nan\n",
      "Epoch: 600, train loss: nan, test loss: nan\n",
      "Epoch: 700, train loss: nan, test loss: nan\n",
      "Epoch: 800, train loss: nan, test loss: nan\n",
      "Epoch: 900, train loss: nan, test loss: nan\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_epochs = 1000 # 1000 epochs\n",
    "learning_rate = 0.001 # 0.001 lr\n",
    "\n",
    "input_size = 22 # number of features\n",
    "hidden_size = 2 # number of features in hidden state\n",
    "num_layers = 1 # number of stacked lstm layers\n",
    "\n",
    "num_classes = 1 # number of output classes \n",
    "\n",
    "lstm = LSTM(num_classes, \n",
    "              input_size, \n",
    "              hidden_size, \n",
    "              num_layers)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimiser = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "training_loop(n_epochs=n_epochs,\n",
    "              lstm=lstm,\n",
    "              optimiser=optimiser,\n",
    "              loss_fn=loss_fn,\n",
    "              X_train=X_train_tensors_final,\n",
    "              y_train=Y_train_tensors,\n",
    "              X_test=X_test_tensors_final,\n",
    "              y_test=Y_test_tensors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
