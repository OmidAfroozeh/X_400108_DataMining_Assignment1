{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_time = 1800\n",
    "\n",
    "# Format into set ready for training and eval\n",
    "df = pd.read_csv(\"feat_eng_\"+str(interval_time)+\".csv\")\n",
    "\n",
    "# Days in the past\n",
    "days_in_past = 0\n",
    "\n",
    "# [{ID, [entries], label}]\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Store max call and sms for later normalization\n",
    "max_call = df[\"call\"].max()\n",
    "max_sms = df[\"sms\"].max()\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "for id in df[\"ID\"].unique():\n",
    "    df_id = df[df[\"ID\"] == id]\n",
    "    for day in df_id[\"timestamp\"].dt.floor(\"D\").unique():\n",
    "        \n",
    "        df_id_days = df_id[((df_id[\"timestamp\"].dt.date <= day.date()) & (df_id[\"timestamp\"].dt.date >= (day - timedelta(days=days_in_past)).date()))].sort_values(by=\"timestamp\", ascending=False)\n",
    "        label = df_id_days[\"next_mood\"].iloc[0]\n",
    "        df_id_days = df_id_days.sort_values(by=\"timestamp\", ascending=True).drop(columns=[\"ID\"])\n",
    "\n",
    "        # being an RNN, timestamp shouldn't be needed since the order is the important\n",
    "        df_id_days[\"timestamp\"] = df_id_days[\"timestamp\"].astype(int) // 10**11\n",
    "        df_id_days[\"timestamp\"] = df_id_days[\"timestamp\"] - df_id_days[\"timestamp\"].min()\n",
    "\n",
    "        df_id_days = df_id_days.drop(columns=\"Unnamed: 0\").reset_index(drop=True)\n",
    "\n",
    "        # Normalize values\n",
    "        # Normalize mood, mood_day-1, mood_day-2, next_mood [1:10]\n",
    "        for col in [\"mood\", \"mood_day-1\", \"mood_day-2\", \"next_mood\"]:\n",
    "            df_id_days[col] = (df_id_days[col] - 1)/(10-1)\n",
    "        # Normalize arousal and valence [-2;2]\n",
    "        df_id_days[\"arousal\"] = (df_id_days[\"arousal\"] - (-2))/(2 - (-2))\n",
    "        df_id_days[\"valence\"] = (df_id_days[\"valence\"] - (-2))/(2 - (-2))\n",
    "        # Activity is already normalized\n",
    "        # Normalize call, sms [0; max_call], [0; max_sms]\n",
    "        df_id_days[\"call\"] = (df_id_days[\"call\"] - 0)/(max_call - 0)\n",
    "        df_id_days[\"sms\"] = (df_id_days[\"sms\"] - 0)/(max_sms - 0)\n",
    "        # Normalize screen, appCat.X\n",
    "        timeBasedFields = ['screen', 'appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office', 'appCat.other','appCat.social', 'appCat.travel', 'appCat.unknown', 'appCat.utilities','appCat.weather']\n",
    "        for col in timeBasedFields:\n",
    "            df_id_days[col] = (df_id_days[col] - 0)/(interval_time - 0)\n",
    "            \n",
    "        # Using only one value since it will be the next day predicted value\n",
    "        X.append(df_id_days.drop(columns=[\"next_mood\", \"timestamp\"]).values)\n",
    "        Y.append(df_id_days[\"next_mood\"][0])\n",
    "\n",
    "        #raw_tuples = list(df_id_days.itertuples(index=False, name=None))\n",
    "        #print(len(raw_tuples[0]))\n",
    "        #data.append({\"ID\": id, \"entries\": raw_tuples, \"label\": label})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Assuming X and Y are lists of values\n",
    "X_array = np.array(X, dtype=np.float32)\n",
    "Y_array = np.array(Y, dtype=np.float32)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_array)\n",
    "Y_tensor = torch.tensor(Y_array)\n",
    "\n",
    "# Removing nan\n",
    "X_tensor = torch.where(torch.isnan(X_tensor), torch.zeros_like(X_tensor), X_tensor)\n",
    "Y_tensor = torch.where(torch.isnan(Y_tensor), torch.zeros_like(Y_tensor), Y_tensor)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "\n",
    "train_size = int(0.75*len(X))\n",
    "test_size = len(X) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "#print(X_tensor.shape)\n",
    "#print(Y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes  # output size\n",
    "        self.num_layers = num_layers  # number of recurrent layers in the lstm\n",
    "        self.input_size = input_size  # input size\n",
    "        self.hidden_size = hidden_size  # neurons in each lstm layer\n",
    "        # LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.2 if num_layers > 1 else 0)\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)  # fully connected \n",
    "        self.fc_2 = nn.Linear(128, num_classes)  # fully connected last layer\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # hidden state\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # cell state\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))  # (input, hidden, and cell state)\n",
    "        hn = hn[-1]  # last layer's hidden state\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out)  # first dense\n",
    "        out = self.relu(out)  # relu\n",
    "        out = self.fc_2(out)  # final output\n",
    "        return out\n",
    "\n",
    "########################################################################################################################################################\n",
    "\n",
    "def training_loop(n_epochs, lstm, optimiser, loss_fn, train_loader, test_loader):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        outputs_return = []\n",
    "        labels_return = []\n",
    "        for inputs, label in train_loader:\n",
    "            # Training mode\n",
    "            lstm.train() \n",
    "            # Reset gradients\n",
    "            optimiser.zero_grad()\n",
    "            # Forward propagation\n",
    "            outputs = lstm(inputs)\n",
    "            # Training loss\n",
    "            loss = loss_fn(outputs, label) \n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            torch.nn.utils.clip_grad_norm_(lstm.parameters(), max_norm=1)\n",
    "            optimiser.step() \n",
    "\n",
    "        # Evaluation mode\n",
    "        lstm.eval() \n",
    "        # Disable gradient calc\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "            # Compute classes and losses\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = lstm(inputs)\n",
    "                outputs_return.append(outputs)\n",
    "                labels_return.append(labels)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}, train loss: {loss.item():.5f}, test loss: {test_loss:.5f}\")\n",
    "            #print(\"Result std: \"+str(np.std(np.array((10-1)*y_test+1))))\n",
    "            #print(\"Prediction std: \"+str(np.std(np.array((10-1)*test_preds+1))))\n",
    "\n",
    "    return outputs_return, labels_return\n",
    "\n",
    "########################################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing learning rate: 0.001, features in hidden layer 5, stacked LSTM 1\n",
      "Epoch: 0, train loss: 0.19640, test loss: 0.18462\n",
      "Epoch: 10, train loss: 0.00082, test loss: 0.00600\n",
      "Epoch: 20, train loss: 0.00062, test loss: 0.00599\n",
      "Epoch: 30, train loss: 0.00118, test loss: 0.00619\n",
      "Epoch: 40, train loss: 0.00099, test loss: 0.00614\n",
      "Epoch: 50, train loss: 0.00083, test loss: 0.00611\n",
      "Epoch: 60, train loss: 0.00081, test loss: 0.00613\n",
      "Epoch: 70, train loss: 0.00071, test loss: 0.00613\n",
      "MSE: 0.5005033754476697\n",
      "MAE: 0.5433870488061354\n",
      "R-squared: 0.028722991107836382\n",
      "Testing learning rate: 0.001, features in hidden layer 5, stacked LSTM 2\n",
      "Epoch: 0, train loss: 0.35767, test loss: 0.33866\n",
      "Epoch: 10, train loss: 0.00087, test loss: 0.00636\n",
      "Epoch: 20, train loss: 0.00095, test loss: 0.00640\n",
      "Epoch: 30, train loss: 0.00104, test loss: 0.00643\n",
      "Epoch: 40, train loss: 0.00087, test loss: 0.00639\n",
      "Epoch: 50, train loss: 0.00106, test loss: 0.00645\n",
      "Epoch: 60, train loss: 0.00100, test loss: 0.00643\n",
      "Epoch: 70, train loss: 0.00089, test loss: 0.00640\n",
      "MSE: 0.5211380639995975\n",
      "MAE: 0.5542978985378375\n",
      "R-squared: -0.011320692030587054\n",
      "Testing learning rate: 0.001, features in hidden layer 5, stacked LSTM 4\n",
      "Epoch: 0, train loss: 0.22208, test loss: 0.20703\n",
      "Epoch: 10, train loss: 0.00104, test loss: 0.00644\n",
      "Epoch: 20, train loss: 0.00086, test loss: 0.00640\n",
      "Epoch: 30, train loss: 0.00099, test loss: 0.00643\n",
      "Epoch: 40, train loss: 0.00099, test loss: 0.00643\n",
      "Epoch: 50, train loss: 0.00144, test loss: 0.00658\n",
      "Epoch: 60, train loss: 0.00092, test loss: 0.00641\n",
      "Epoch: 70, train loss: 0.00182, test loss: 0.00674\n",
      "MSE: 0.5249739108451985\n",
      "MAE: 0.559959629120735\n",
      "R-squared: -0.018764537633888745\n",
      "Testing learning rate: 0.001, features in hidden layer 15, stacked LSTM 1\n",
      "Epoch: 0, train loss: 0.18708, test loss: 0.17907\n",
      "Epoch: 10, train loss: 0.00070, test loss: 0.00599\n",
      "Epoch: 20, train loss: 0.00074, test loss: 0.00604\n",
      "Epoch: 30, train loss: 0.00101, test loss: 0.00618\n",
      "Epoch: 40, train loss: 0.00063, test loss: 0.00611\n",
      "Epoch: 50, train loss: 0.00098, test loss: 0.00622\n",
      "Epoch: 60, train loss: 0.00114, test loss: 0.00630\n",
      "Epoch: 70, train loss: 0.00099, test loss: 0.00627\n",
      "MSE: 0.5010360255871481\n",
      "MAE: 0.5398541393761451\n",
      "R-squared: 0.027689330078486063\n",
      "Testing learning rate: 0.001, features in hidden layer 15, stacked LSTM 2\n",
      "Epoch: 0, train loss: 0.07919, test loss: 0.07321\n",
      "Epoch: 10, train loss: 0.00106, test loss: 0.00606\n",
      "Epoch: 20, train loss: 0.00081, test loss: 0.00607\n",
      "Epoch: 30, train loss: 0.00112, test loss: 0.00623\n",
      "Epoch: 40, train loss: 0.00068, test loss: 0.00616\n",
      "Epoch: 50, train loss: 0.00067, test loss: 0.00619\n",
      "Epoch: 60, train loss: 0.00106, test loss: 0.00632\n",
      "Epoch: 70, train loss: 0.00228, test loss: 0.00687\n",
      "MSE: 0.5060629140674069\n",
      "MAE: 0.5428442419148408\n",
      "R-squared: 0.017934148701790664\n",
      "Testing learning rate: 0.001, features in hidden layer 15, stacked LSTM 4\n",
      "Epoch: 0, train loss: 0.21792, test loss: 0.20290\n",
      "Epoch: 10, train loss: 0.00090, test loss: 0.00635\n",
      "Epoch: 20, train loss: 0.00111, test loss: 0.00641\n",
      "Epoch: 30, train loss: 0.00091, test loss: 0.00636\n",
      "Epoch: 40, train loss: 0.00053, test loss: 0.00633\n",
      "Epoch: 50, train loss: 0.00043, test loss: 0.00634\n",
      "Epoch: 60, train loss: 0.00073, test loss: 0.00635\n",
      "Epoch: 70, train loss: 0.00086, test loss: 0.00637\n",
      "MSE: 0.5154762697374031\n",
      "MAE: 0.5493607856333256\n",
      "R-squared: -0.00033341229240679837\n",
      "Testing learning rate: 0.001, features in hidden layer 25, stacked LSTM 1\n",
      "Epoch: 0, train loss: 0.14195, test loss: 0.13830\n",
      "Epoch: 10, train loss: 0.00006, test loss: 0.00501\n",
      "Epoch: 20, train loss: 0.00059, test loss: 0.00564\n",
      "Epoch: 30, train loss: 0.00012, test loss: 0.00577\n",
      "Epoch: 40, train loss: 0.00146, test loss: 0.00649\n",
      "Epoch: 50, train loss: 0.00034, test loss: 0.00604\n",
      "Epoch: 60, train loss: 0.00086, test loss: 0.00621\n",
      "Epoch: 70, train loss: 0.00100, test loss: 0.00631\n",
      "MSE: 0.5029722834103237\n",
      "MAE: 0.5452168586735542\n",
      "R-squared: 0.023931827533660965\n",
      "Testing learning rate: 0.001, features in hidden layer 25, stacked LSTM 2\n",
      "Epoch: 0, train loss: 0.20370, test loss: 0.18764\n",
      "Epoch: 10, train loss: 0.00120, test loss: 0.00576\n",
      "Epoch: 20, train loss: 0.00041, test loss: 0.00589\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "# Interval time: 1800, 7200, 21600, 36000\n",
    "# Learning rate 0.1, 0.05, 0.001, 0.0005\n",
    "# Hidden_size 2, 4, 10, 15, 25, 35\n",
    "# Num layers 1, 2, 4,8\n",
    "\n",
    "n_epochs = 80 # 1000 epochs\n",
    "learning_rates = [0.001]\n",
    "\n",
    "input_size = 21 # number of features\n",
    "hidden_sizes = [5, 15, 25] # number of features in hidden state\n",
    "num_layers_list = [1, 2, 4] # number of stacked lstm layers\n",
    "\n",
    "num_classes = 1 # number of output classes \n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for num_layers in num_layers_list:\n",
    "            print(f\"Testing learning rate: {learning_rate}, features in hidden layer {hidden_size}, stacked LSTM {num_layers}\")\n",
    "\n",
    "            folder = \"./pt\"+str(interval_time)+\"_lr\"+str(learning_rate).replace(\".\", \"dot\")+\"_hs\"+str(hidden_size)+\"_nl\"+str(num_layers)+\"/\"\n",
    "            os.makedirs(folder)\n",
    "\n",
    "            lstm = LSTM(num_classes, \n",
    "                        input_size, \n",
    "                        hidden_size, \n",
    "                        num_layers)\n",
    "\n",
    "            loss_fn = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "            optimiser = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "            prediction, result = training_loop(n_epochs=n_epochs,\n",
    "                        lstm=lstm,\n",
    "                        optimiser=optimiser,\n",
    "                        loss_fn=loss_fn,\n",
    "                        train_loader = train_loader,\n",
    "                        test_loader = test_loader)\n",
    "\n",
    "            prediction = np.array([tensor.item() for tensor in prediction])\n",
    "            result = np.array([tensor.item() for tensor in result])\n",
    "            prediction = (10-1)*prediction+1\n",
    "            result = (10-1)*result+1\n",
    "\n",
    "            #print(\"Expected std: \"+str(np.std(result)))\n",
    "            #print(\"Infer std: \"+str(np.std(prediction)))\n",
    "            #\n",
    "            #print(\"Learning rate: \"+str(learning_rate))\n",
    "            #print(\"Hidden Size: \"+str(hidden_size))\n",
    "            #print(\"LSTM layers: \"+str(num_layers))\n",
    "            #print(\"Loss function: MSE\")\n",
    "            #\n",
    "            #print(\"Number of different test predictions: \"+str(len(np.unique(prediction))))\n",
    "            #print(\"First 10 values: \"+str(prediction[:10]))\n",
    "\n",
    "            np.savetxt(folder+\"result.txt\", result, fmt='%.2f')\n",
    "            np.savetxt(folder+\"prediction.txt\", prediction, fmt='%.2f')\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            delta = abs(prediction - result)\n",
    "            mse = np.mean(np.square(delta))\n",
    "            mae = np.mean(delta)\n",
    "            delta = np.sort(delta)\n",
    "\n",
    "            # Compute percentiles\n",
    "            percentile_25 = np.percentile(delta, 25)\n",
    "            median = np.percentile(delta, 50)\n",
    "            percentile_75 = np.percentile(delta, 75)\n",
    "\n",
    "            # Plot the sorted data\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(range(len(delta)), delta, marker='o', linestyle='-')\n",
    "            plt.ylabel('Difference')\n",
    "            plt.xlabel('Testcase index')\n",
    "            plt.title('Label value-Predicted value difference')\n",
    "\n",
    "            # Plot percentiles\n",
    "            plt.axhline(y=percentile_25, color='r', linestyle='--', label='25th percentile')\n",
    "            plt.axhline(y=median, color='g', linestyle='--', label='Median')\n",
    "            plt.axhline(y=percentile_75, color='b', linestyle='--', label='75th percentile')\n",
    "\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            #plt.show()\n",
    "\n",
    "            #print(\"25th percentile:\", percentile_25)\n",
    "            #print(\"Median:\", median)\n",
    "            #print(\"75th percentile:\", percentile_75)\n",
    "            print(\"MSE: \"+str(mse))\n",
    "            print(\"MAE: \"+str(mae))\n",
    "\n",
    "            plt.savefig(folder+\"plot_original.png\")\n",
    "            np.savetxt(folder+\"MSE_original.txt\", np.array([mse]), fmt='%.3f')\n",
    "            np.savetxt(folder+\"MAE_original.txt\", np.array([mae]), fmt='%.3f')\n",
    "            # Round the arrays\n",
    "            prediction_rounded = np.round(prediction)\n",
    "            result_rounded = np.round(result)\n",
    "\n",
    "            # Compute differences\n",
    "            delta = np.abs(prediction_rounded - result_rounded)\n",
    "            mse = np.mean(np.square(delta))\n",
    "            mae = np.mean(delta)\n",
    "            delta_sorted = np.sort(delta)\n",
    "\n",
    "            # Compute percentiles\n",
    "            percentile_25 = np.percentile(delta_sorted, 25)\n",
    "            median = np.percentile(delta_sorted, 50)\n",
    "            percentile_75 = np.percentile(delta_sorted, 75)\n",
    "\n",
    "            # Plot the sorted data\n",
    "            #plt.figure(figsize=(8, 6))\n",
    "            #plt.plot(range(len(delta_sorted)), delta_sorted, marker='o', linestyle='-')\n",
    "            #plt.ylabel('Difference')\n",
    "            #plt.xlabel('Testcase index')\n",
    "            #plt.title('Label value - Predicted value difference (Rounded)')\n",
    "#\n",
    "            #plt.legend()\n",
    "            #plt.grid(True)\n",
    "            #plt.show()\n",
    "            #\n",
    "            #print(\"25th percentile:\", percentile_25)\n",
    "            #print(\"Median:\", median)\n",
    "            #print(\"75th percentile:\", percentile_75)\n",
    "            #print(\"MSE:\", mse)\n",
    "\n",
    "            #plt.savefig(folder+\"plot_rounded.png\")\n",
    "            np.savetxt(folder+\"MSE_rounded.txt\", np.array([mse]), fmt='%.3f')\n",
    "            np.savetxt(folder+\"MAE_rounded.txt\", np.array([mae]), fmt='%.3f')\n",
    "\n",
    "            result_mean = np.mean(result)\n",
    "\n",
    "            # Compute the sum of squares of residuals (SS_res)\n",
    "            ss_res = np.sum((result - prediction) ** 2)\n",
    "\n",
    "            # Compute the total sum of squares (SS_tot)\n",
    "            ss_tot = np.sum((result - result_mean) ** 2)\n",
    "\n",
    "            # Compute R-squared\n",
    "            r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "            print(\"R-squared:\", r_squared)\n",
    "\n",
    "            np.savetxt(folder+\"R_squared.txt\", np.array([r_squared]), fmt='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
