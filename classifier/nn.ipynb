{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_time = 1800\n",
    "\n",
    "# Format into set ready for training and eval\n",
    "df = pd.read_csv(\"feat_eng_\"+str(interval_time)+\".csv\")\n",
    "\n",
    "# Days in the past\n",
    "days_in_past = 0\n",
    "\n",
    "# [{ID, [entries], label}]\n",
    "data = []\n",
    "X = dict()\n",
    "Y = dict()\n",
    "\n",
    "# Store max call and sms for later normalization\n",
    "max_call = df[\"call\"].max()\n",
    "max_sms = df[\"sms\"].max()\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "for id in df[\"ID\"].unique():\n",
    "    X[id] = dict()\n",
    "    Y[id] = dict()\n",
    "    df_id = df[df[\"ID\"] == id]\n",
    "    for day in df_id[\"timestamp\"].dt.floor(\"D\").unique():\n",
    "        \n",
    "        df_id_days = df_id[((df_id[\"timestamp\"].dt.date <= day.date()) & (df_id[\"timestamp\"].dt.date >= (day - timedelta(days=days_in_past)).date()))].sort_values(by=\"timestamp\", ascending=False)\n",
    "        label = df_id_days[\"next_mood\"].iloc[0]\n",
    "        df_id_days = df_id_days.sort_values(by=\"timestamp\", ascending=True).drop(columns=[\"ID\"])\n",
    "\n",
    "        # being an RNN, timestamp shouldn't be needed since the order is the important\n",
    "        df_id_days[\"timestamp\"] = df_id_days[\"timestamp\"].astype(int) // 10**11\n",
    "        df_id_days[\"timestamp\"] = df_id_days[\"timestamp\"] - df_id_days[\"timestamp\"].min()\n",
    "\n",
    "        df_id_days = df_id_days.drop(columns=\"Unnamed: 0\").reset_index(drop=True)\n",
    "\n",
    "        # Normalize values\n",
    "        # Normalize mood, mood_day-1, mood_day-2, next_mood [1:10]\n",
    "        for col in [\"mood\", \"mood_day-1\", \"mood_day-2\", \"next_mood\"]:\n",
    "            df_id_days[col] = (df_id_days[col] - 1)/(10-1)\n",
    "        # Normalize arousal and valence [-2;2]\n",
    "        df_id_days[\"arousal\"] = (df_id_days[\"arousal\"] - (-2))/(2 - (-2))\n",
    "        df_id_days[\"valence\"] = (df_id_days[\"valence\"] - (-2))/(2 - (-2))\n",
    "        # Activity is already normalized\n",
    "        # Normalize call, sms [0; max_call], [0; max_sms]\n",
    "        df_id_days[\"call\"] = (df_id_days[\"call\"] - 0)/(max_call - 0)\n",
    "        df_id_days[\"sms\"] = (df_id_days[\"sms\"] - 0)/(max_sms - 0)\n",
    "        # Normalize screen, appCat.X\n",
    "        timeBasedFields = ['screen', 'appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office', 'appCat.other','appCat.social', 'appCat.travel', 'appCat.unknown', 'appCat.utilities','appCat.weather']\n",
    "        for col in timeBasedFields:\n",
    "            df_id_days[col] = (df_id_days[col] - 0)/(interval_time - 0)\n",
    "            \n",
    "        # Using only one value since it will be the next day predicted value\n",
    "        X[id][day] , Y[id][day] = df_id_days.drop(columns=[\"next_mood\", \"timestamp\"]), df_id_days[\"next_mood\"][0]\n",
    "        #raw_tuples = list(df_id_days.itertuples(index=False, name=None))\n",
    "        #print(len(raw_tuples[0]))\n",
    "        #data.append({\"ID\": id, \"entries\": raw_tuples, \"label\": label})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ids for training: 20\n",
      "Number of ids for testing: 7\n",
      "(939, 48, 21)\n",
      "(939,)\n",
      "(306, 48, 21)\n",
      "(306,)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "train_ids = random.sample(list(X.keys()), int(len(X)*0.75))\n",
    "print(\"Number of ids for training: \"+str(len(train_ids)))\n",
    "print(\"Number of ids for testing: \"+ str(len(X) - len(train_ids)))\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for id in X.keys():\n",
    "    is_train = True if id in train_ids else False\n",
    "    for day in X[id].keys():\n",
    "        if is_train:\n",
    "            X_train.append(X[id][day])\n",
    "            Y_train.append(Y[id][day])\n",
    "        else:\n",
    "            X_test.append(X[id][day])\n",
    "            Y_test.append(Y[id][day])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([939, 48, 21])\n",
      "torch.Size([306, 48, 21])\n",
      "torch.Size([939])\n",
      "torch.Size([306])\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "Training Shape: torch.Size([939, 48, 21]) torch.Size([939])\n",
      "Testing Shape: torch.Size([306, 48, 21]) torch.Size([306])\n"
     ]
    }
   ],
   "source": [
    "# Convert to pytorch tensors\n",
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "\n",
    "Y_train_tensors = Variable(torch.Tensor(Y_train))\n",
    "Y_test_tensors = Variable(torch.Tensor(Y_test))\n",
    "\n",
    "print(X_train_tensors.shape)\n",
    "print(X_test_tensors.shape) \n",
    "\n",
    "print(Y_train_tensors.shape)\n",
    "print(Y_test_tensors.shape) \n",
    "\n",
    "# Reshaping to rows, timestamps, features\n",
    "X_train_tensors_final = torch.reshape(X_train_tensors,   \n",
    "                                      (X_train_tensors.shape[0], 48, \n",
    "                                       X_train_tensors.shape[2]))\n",
    "X_test_tensors_final = torch.reshape(X_test_tensors,  \n",
    "                                     (X_test_tensors.shape[0], 48, \n",
    "                                      X_test_tensors.shape[2])) \n",
    "\n",
    "\n",
    "\n",
    "# Apparently there are some nan values in training tensors\n",
    "\n",
    "print(torch.isnan(X_train_tensors_final).any())\n",
    "print(torch.isnan(Y_train_tensors).any())\n",
    "print(torch.isnan(X_test_tensors_final).any())\n",
    "print(torch.isnan(Y_test_tensors).any())\n",
    "\n",
    "\n",
    "\n",
    "# Removing nan\n",
    "X_train_tensors_final = torch.where(torch.isnan(X_train_tensors_final), torch.zeros_like(X_train_tensors_final), X_train_tensors_final)\n",
    "\n",
    "X_test_tensors_final = torch.where(torch.isnan(X_test_tensors_final), torch.zeros_like(X_test_tensors_final), X_test_tensors_final)\n",
    "\n",
    "\n",
    "print(\"Training Shape:\", X_train_tensors_final.shape, Y_train_tensors.shape)\n",
    "print(\"Testing Shape:\", X_test_tensors_final.shape, Y_test_tensors.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.43202, test loss: 0.45549\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.02107735\n",
      "Epoch: 100, train loss: 0.22586, test loss: 0.24218\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.065615095\n",
      "Epoch: 200, train loss: 0.00781, test loss: 0.00821\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.2743955\n",
      "Epoch: 300, train loss: 0.00753, test loss: 0.00810\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.23903398\n",
      "Epoch: 400, train loss: 0.00733, test loss: 0.00791\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.21309392\n",
      "Epoch: 500, train loss: 0.00717, test loss: 0.00775\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.18890655\n",
      "Epoch: 600, train loss: 0.00705, test loss: 0.00763\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.16665898\n",
      "Epoch: 700, train loss: 0.00695, test loss: 0.00753\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.14657158\n",
      "Epoch: 800, train loss: 0.00687, test loss: 0.00746\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.12867999\n",
      "Epoch: 900, train loss: 0.00681, test loss: 0.00740\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.11309389\n",
      "Epoch: 1000, train loss: 0.00677, test loss: 0.00736\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.099736355\n",
      "Epoch: 1100, train loss: 0.00674, test loss: 0.00732\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.088387\n",
      "Epoch: 1200, train loss: 0.00672, test loss: 0.00730\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.07881827\n",
      "Epoch: 1300, train loss: 0.00670, test loss: 0.00729\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.07080989\n",
      "Epoch: 1400, train loss: 0.00668, test loss: 0.00727\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.0641852\n",
      "Epoch: 1500, train loss: 0.00667, test loss: 0.00727\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.058692273\n",
      "Epoch: 1600, train loss: 0.00667, test loss: 0.00726\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.054056324\n",
      "Epoch: 1700, train loss: 0.00666, test loss: 0.00725\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.050081044\n",
      "Epoch: 1800, train loss: 0.00665, test loss: 0.00725\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.046578616\n",
      "Epoch: 1900, train loss: 0.00665, test loss: 0.00725\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.043467555\n",
      "Epoch: 2000, train loss: 0.00665, test loss: 0.00725\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.04066874\n",
      "Epoch: 2100, train loss: 0.00664, test loss: 0.00724\n",
      "Result std: 0.7460366\n",
      "Prediction std: 0.038127095\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()    \u001b[38;5;66;03m# mean-squared error for regression\u001b[39;00m\n\u001b[1;32m     89\u001b[0m optimiser \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(lstm\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m---> 91\u001b[0m prediction, result \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m              \u001b[49m\u001b[43mlstm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m              \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m              \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m              \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_tensors_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m              \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m              \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test_tensors_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m              \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_test_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m prediction \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mprediction\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    101\u001b[0m result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mresult\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[160], line 55\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(n_epochs, lstm, optimiser, loss_fn, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Disable gradient calc\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Compute classes and losses\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     test_preds \u001b[38;5;241m=\u001b[39m \u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m loss_fn(test_preds, y_test)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[160], line 22\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m c_0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Propagate input through LSTM\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m output, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (input, hidden, and cell state)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m hn \u001b[38;5;241m=\u001b[39m hn[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# last layer's hidden state\u001b[39;00m\n\u001b[1;32m     24\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(hn)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes  # output size\n",
    "        self.num_layers = num_layers  # number of recurrent layers in the lstm\n",
    "        self.input_size = input_size  # input size\n",
    "        self.hidden_size = hidden_size  # neurons in each lstm layer\n",
    "        # LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.2 if num_layers > 1 else 0)\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)  # fully connected \n",
    "        self.fc_2 = nn.Linear(128, num_classes)  # fully connected last layer\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # hidden state\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # cell state\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))  # (input, hidden, and cell state)\n",
    "        hn = hn[-1]  # last layer's hidden state\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out)  # first dense\n",
    "        out = self.relu(out)  # relu\n",
    "        out = self.fc_2(out)  # final output\n",
    "        return out\n",
    "\n",
    "########################################################################################################################################################\n",
    "\n",
    "def training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Training mode\n",
    "        lstm.train() \n",
    "        # Reset gradients\n",
    "        optimiser.zero_grad()\n",
    "        # Forward propagation\n",
    "        outputs = lstm(X_train)\n",
    "        # Training loss\n",
    "        loss = loss_fn(outputs, y_train) \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        torch.nn.utils.clip_grad_norm_(lstm.parameters(), max_norm=1)\n",
    "        optimiser.step() \n",
    "\n",
    "        # Evaluation mode\n",
    "        lstm.eval() \n",
    "        # Disable gradient calc\n",
    "        with torch.no_grad():\n",
    "            # Compute classes and losses\n",
    "            test_preds = lstm(X_test)\n",
    "            test_loss = loss_fn(test_preds, y_test)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, train loss: {loss.item():.5f}, test loss: {test_loss.item():.5f}\")\n",
    "            print(\"Result std: \"+str(np.std(np.array((10-1)*y_test+1))))\n",
    "            print(\"Prediction std: \"+str(np.std(np.array((10-1)*test_preds+1))))\n",
    "\n",
    "    return test_preds, y_test\n",
    "\n",
    "########################################################################################################################################################\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Learning rate 0.01, 0.005, 0.001, 0.0005\n",
    "# Hidden_size 2, 3, 4, 6, 8, 10, 12, 14, 18, 22, 25, 30\n",
    "# Num layers 1, 2, 3, 4, 5, 6\n",
    "# Loss functions  L1Loss, MSELoss\n",
    "\n",
    "n_epochs = 1000 # 1000 epochs\n",
    "learning_rate = 0.001 # 0.001 lr\n",
    "\n",
    "input_size = 21 # number of features\n",
    "hidden_size = 15 # number of features in hidden state\n",
    "num_layers = 1 # number of stacked lstm layers\n",
    "\n",
    "num_classes = 1 # number of output classes \n",
    "\n",
    "lstm = LSTM(num_classes, \n",
    "              input_size, \n",
    "              hidden_size, \n",
    "              num_layers)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimiser = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "prediction, result = training_loop(n_epochs=n_epochs,\n",
    "              lstm=lstm,\n",
    "              optimiser=optimiser,\n",
    "              loss_fn=loss_fn,\n",
    "              X_train=X_train_tensors_final,\n",
    "              y_train=Y_train_tensors,\n",
    "              X_test=X_test_tensors_final,\n",
    "              y_test=Y_test_tensors)\n",
    "\n",
    "prediction = (10-1)*prediction+1\n",
    "result = (10-1)*result+1\n",
    "\n",
    "print(\"Expected std: \"+str(np.std(np.array(result))))\n",
    "print(\"Infer std: \"+str(np.std(np.array(prediction))))\n",
    "\n",
    "print(\"Learning rate: \"+str(learning_rate))\n",
    "print(\"Hidden Size: \"+str(hidden_size))\n",
    "print(\"LSTM layers: \"+str(num_layers))\n",
    "print(\"Loss function: MSE\")\n",
    "\n",
    "print(\"Number of different test predictions: \"+str(len(np.unique(prediction.numpy()))))\n",
    "print(\"First 10 values: \"+str(prediction.numpy()[:10]))\n",
    "\n",
    "########################################################################################################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate absolute differences\n",
    "differences = [abs(e - m) for e, m in zip(result, prediction)]\n",
    "\n",
    "# Sort indices based on differences\n",
    "sorted_indices = sorted(range(len(differences)), key=lambda k: differences[k])\n",
    "\n",
    "# Reorder lists based on sorted indices\n",
    "expected_output_sorted = [prediction[i] for i in sorted_indices]\n",
    "model_results_sorted = [result[i] for i in sorted_indices]\n",
    "differences_sorted = [differences[i] for i in sorted_indices]\n",
    "\n",
    "# Plot the points\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(range(len(expected_output_sorted)), expected_output_sorted, label='Expected Output')\n",
    "plt.scatter(range(len(model_results_sorted)), model_results_sorted, label='Model Results')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Expected Output vs. Model Results (Ordered by Difference)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoodLSTMRegression(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(nn.LSTM, self)__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)         # 10 for 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        out, (_, _) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MoodLSTMRegression(input_dim = 19, hidden_dim = 10, num_layers = 1, output_dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss_result = []\n",
    "for epoch in range(100):\n",
    "    for inputs, labels in train_loader: \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backwards()\n",
    "        optimizer.step()\n",
    "        loss_result.append(loss.item())\n",
    "        print(f\"Epoch {epoch+1}, Loss {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
