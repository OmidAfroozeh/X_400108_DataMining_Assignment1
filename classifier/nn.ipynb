{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format into set ready for training and eval\n",
    "df = pd.read_csv(\"feat_eng_1800.csv\")\n",
    "\n",
    "# Days in the past\n",
    "days_in_past = 0\n",
    "\n",
    "# [{ID, [entries], label}]\n",
    "data = []\n",
    "X = dict()\n",
    "Y = dict()\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "for id in df[\"ID\"].unique():\n",
    "    X[id] = dict()\n",
    "    Y[id] = dict()\n",
    "    df_id = df[df[\"ID\"] == id]\n",
    "    for day in df_id[\"timestamp\"].dt.floor(\"D\").unique():\n",
    "        \n",
    "        df_id_days = df_id[((df_id[\"timestamp\"].dt.date <= day.date()) & (df_id[\"timestamp\"].dt.date >= (day - timedelta(days=days_in_past)).date()))].sort_values(by=\"timestamp\", ascending=False)\n",
    "        label = df_id_days[\"next_mood\"].iloc[0]\n",
    "        df_id_days = df_id_days.sort_values(by=\"timestamp\", ascending=True).drop(columns=[\"ID\"])\n",
    "\n",
    "        # being an RNN, timestamp shouldn't be needed since the order is the important\n",
    "        df_id_days[\"timestamp\"] = df_id_days[\"timestamp\"].astype(int) // 10**11\n",
    "        df_id_days[\"timestamp\"] = df_id_days[\"timestamp\"] - df_id_days[\"timestamp\"].min()\n",
    "\n",
    "        df_id_days = df_id_days.drop(columns=\"Unnamed: 0\").reset_index(drop=True)\n",
    "\n",
    "        # Using only one value since it will be the next day predicted value\n",
    "        X[id][day] , Y[id][day] = df_id_days.drop(columns=\"next_mood\"), df_id_days[\"next_mood\"][0]\n",
    "\n",
    "        #raw_tuples = list(df_id_days.itertuples(index=False, name=None))\n",
    "        #print(len(raw_tuples[0]))\n",
    "        #data.append({\"ID\": id, \"entries\": raw_tuples, \"label\": label})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ids for training: 20\n",
      "Number of ids for testing: 7\n",
      "(930, 48, 22)\n",
      "(930,)\n",
      "(315, 48, 22)\n",
      "(315,)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "train_ids = random.sample(list(X.keys()), int(len(X)*0.75))\n",
    "print(\"Number of ids for training: \"+str(len(train_ids)))\n",
    "print(\"Number of ids for testing: \"+ str(len(X) - len(train_ids)))\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for id in X.keys():\n",
    "    is_train = True if id in train_ids else False\n",
    "    for day in X[id].keys():\n",
    "        if is_train:\n",
    "            X_train.append(X[id][day])\n",
    "            Y_train.append(Y[id][day])\n",
    "        else:\n",
    "            X_test.append(X[id][day])\n",
    "            Y_test.append(Y[id][day])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([930, 48, 22])\n",
      "torch.Size([315, 48, 22])\n",
      "torch.Size([930])\n",
      "torch.Size([315])\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "Training Shape: torch.Size([930, 48, 22]) torch.Size([930])\n",
      "Testing Shape: torch.Size([315, 48, 22]) torch.Size([315])\n"
     ]
    }
   ],
   "source": [
    "# Convert to pytorch tensors\n",
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "\n",
    "Y_train_tensors = Variable(torch.Tensor(Y_train))\n",
    "Y_test_tensors = Variable(torch.Tensor(Y_test))\n",
    "\n",
    "print(X_train_tensors.shape)\n",
    "print(X_test_tensors.shape) \n",
    "\n",
    "print(Y_train_tensors.shape)\n",
    "print(Y_test_tensors.shape) \n",
    "\n",
    "# Reshaping to rows, timestamps, features\n",
    "X_train_tensors_final = torch.reshape(X_train_tensors,   \n",
    "                                      (X_train_tensors.shape[0], 48, \n",
    "                                       X_train_tensors.shape[2]))\n",
    "X_test_tensors_final = torch.reshape(X_test_tensors,  \n",
    "                                     (X_test_tensors.shape[0], 48, \n",
    "                                      X_test_tensors.shape[2])) \n",
    "\n",
    "\n",
    "\n",
    "# Apparently there are some nan values in training tensors\n",
    "\n",
    "print(torch.isnan(X_train_tensors_final).any())\n",
    "print(torch.isnan(Y_train_tensors).any())\n",
    "print(torch.isnan(X_test_tensors_final).any())\n",
    "print(torch.isnan(Y_test_tensors).any())\n",
    "\n",
    "\n",
    "\n",
    "# Removing nan\n",
    "X_train_tensors_final = torch.where(torch.isnan(X_train_tensors_final), torch.zeros_like(X_train_tensors_final), X_train_tensors_final)\n",
    "\n",
    "X_test_tensors_final = torch.where(torch.isnan(X_test_tensors_final), torch.zeros_like(X_test_tensors_final), X_test_tensors_final)\n",
    "\n",
    "\n",
    "print(\"Training Shape:\", X_train_tensors_final.shape, Y_train_tensors.shape)\n",
    "print(\"Testing Shape:\", X_test_tensors_final.shape, Y_test_tensors.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes  # output size\n",
    "        self.num_layers = num_layers  # number of recurrent layers in the lstm\n",
    "        self.input_size = input_size  # input size\n",
    "        self.hidden_size = hidden_size  # neurons in each lstm layer\n",
    "        # LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.2 if num_layers > 1 else 0)\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)  # fully connected \n",
    "        self.fc_2 = nn.Linear(128, num_classes)  # fully connected last layer\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # hidden state\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # cell state\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))  # (input, hidden, and cell state)\n",
    "        hn = hn[-1]  # last layer's hidden state\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out)  # first dense\n",
    "        out = self.relu(out)  # relu\n",
    "        out = self.fc_2(out)  # final output\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Training mode\n",
    "        lstm.train() \n",
    "        # Reset gradients\n",
    "        optimiser.zero_grad()\n",
    "        # Forward propagation\n",
    "        outputs = lstm(X_train)\n",
    "        # Training loss\n",
    "        loss = loss_fn(outputs, y_train) \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        torch.nn.utils.clip_grad_norm_(lstm.parameters(), max_norm=1)\n",
    "        optimiser.step() \n",
    "\n",
    "        # Evaluation mode\n",
    "        lstm.eval() \n",
    "        # Disable gradient calc\n",
    "        with torch.no_grad():\n",
    "            # Compute classes and losses\n",
    "            test_preds = lstm(X_test)\n",
    "            test_loss = loss_fn(test_preds, y_test)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, train loss: {loss.item():.5f}, test loss: {test_loss.item():.5f}\")\n",
    "\n",
    "    return test_preds, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 51.16244, test loss: 50.89769\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Learning rate 0.01, 0.005, 0.001, 0.0005\n",
    "# Hidden_size 2, 3, 4, 6, 8, 10, 12, 14, 18, 22, 25, 30\n",
    "# Num layers 1, 2, 3, 4, 5, 6\n",
    "# Loss functions  L1Loss, MSELoss\n",
    "\n",
    "n_epochs = 1000 # 1000 epochs\n",
    "learning_rate = 0.001 # 0.001 lr\n",
    "\n",
    "input_size = 22 # number of features\n",
    "hidden_size = 2 # number of features in hidden state\n",
    "num_layers = 1 # number of stacked lstm layers\n",
    "\n",
    "num_classes = 1 # number of output classes \n",
    "\n",
    "lstm = LSTM(num_classes, \n",
    "              input_size, \n",
    "              hidden_size, \n",
    "              num_layers)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimiser = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "prediction, result = training_loop(n_epochs=n_epochs,\n",
    "              lstm=lstm,\n",
    "              optimiser=optimiser,\n",
    "              loss_fn=loss_fn,\n",
    "              X_train=X_train_tensors_final,\n",
    "              y_train=Y_train_tensors,\n",
    "              X_test=X_test_tensors_final,\n",
    "              y_test=Y_test_tensors)\n",
    "\n",
    "print(\"Learning rate: \"+str(learning_rate))\n",
    "print(\"Hidden Size: \"+str(hidden_size))\n",
    "print(\"LSTM layers: \"+str(num_layers))\n",
    "print(\"Loss function: MSE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Calculate absolute differences\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m differences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mabs\u001b[39m(e \u001b[38;5;241m-\u001b[39m m) \u001b[38;5;28;01mfor\u001b[39;00m e, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, prediction)]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Sort indices based on differences\u001b[39;00m\n\u001b[1;32m      7\u001b[0m sorted_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(differences)), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m k: differences[k])\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.0.7/libexec/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.0.7/libexec/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate absolute differences\n",
    "differences = [abs(e - m) for e, m in zip(result, prediction)]\n",
    "\n",
    "# Sort indices based on differences\n",
    "sorted_indices = sorted(range(len(differences)), key=lambda k: differences[k])\n",
    "\n",
    "# Reorder lists based on sorted indices\n",
    "expected_output_sorted = [prediction[i] for i in sorted_indices]\n",
    "model_results_sorted = [result[i] for i in sorted_indices]\n",
    "differences_sorted = [differences[i] for i in sorted_indices]\n",
    "\n",
    "# Plot the points\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(range(len(expected_output_sorted)), expected_output_sorted, label='Expected Output')\n",
    "plt.scatter(range(len(model_results_sorted)), model_results_sorted, label='Model Results')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Expected Output vs. Model Results (Ordered by Difference)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
